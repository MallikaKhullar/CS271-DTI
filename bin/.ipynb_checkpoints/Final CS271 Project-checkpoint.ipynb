{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, BatchSampler, SequentialSampler\n",
    "\n",
    "from evaluation_metrics import prec_rec_f1_acc_mcc, get_list_of_scores\n",
    "\n",
    "project_file_path = \"{}CS271-DTI\".format(os.getcwd().split(\"CS271-DTI\")[0])\n",
    "training_files_path = \"{}/training_files\".format(project_file_path)\n",
    "\n",
    "# print(project_file_path, training_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataSet(Dataset):\n",
    "    def __init__(self, target_id, train_val_test):\n",
    "        print(\"Fetching {} dataset for Target: {}\".format(train_val_test, target_id))\n",
    "        self.target_id = target_id\n",
    "        self.train_val_test = train_val_test\n",
    "        self.training_dataset_path = \"{}/{}\".format(training_files_path, target_id)\n",
    "\n",
    "        #         print(self.training_dataset_path)\n",
    "\n",
    "        self.train_val_test_folds = json.load(open(os.path.join(self.training_dataset_path, \"train_val_test_dict.json\")))\n",
    "        self.compid_list = [compid_label[0] for compid_label in self.train_val_test_folds[train_val_test]]\n",
    "        self.label_list = [compid_label[1] for compid_label in self.train_val_test_folds[train_val_test]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.compid_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comp_id = self.compid_list[index]\n",
    "        img_path = os.path.join(self.training_dataset_path, \"imgs\", \"{}.png\".format(comp_id))\n",
    "        img_arr = cv2.imread(img_path)\n",
    "        if random.random()>=0.50:\n",
    "            angle = random.randint(0,359)\n",
    "            rows, cols, channel = img_arr.shape\n",
    "            rotation_matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "            img_arr = cv2.warpAffine(img_arr, rotation_matrix, (cols, rows), cv2.INTER_LINEAR,\n",
    "                                                 borderValue=(255, 255, 255))\n",
    "        img_arr = np.array(img_arr) / 255.0\n",
    "        img_arr = img_arr.transpose((2, 0, 1))\n",
    "        label = self.label_list[index]\n",
    "        return img_arr, label, comp_id\n",
    "\n",
    "\n",
    "def get_dataLoader(target_id, batch_size=32):\n",
    "    training_dataset = DataSet(target_id, \"training\")\n",
    "    validation_dataset = DataSet(target_id, \"validation\")\n",
    "    test_dataset = DataSet(target_id, \"test\")\n",
    "    train_sampler = SubsetRandomSampler(range(len(training_dataset)))\n",
    "    train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size,\n",
    "                                              sampler=train_sampler)\n",
    "\n",
    "    validation_sampler = SubsetRandomSampler(range(len(validation_dataset)))\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size,\n",
    "                                               sampler=validation_sampler)\n",
    "\n",
    "    test_sampler = SubsetRandomSampler(range(len(test_dataset)))\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                               sampler=test_sampler)\n",
    "\n",
    "    return train_loader, validation_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from operator import itemgetter\n",
    "\n",
    "class CNNModel1(nn.Module):\n",
    "    def __init__(self, fully_layer_1, fully_layer_2, drop_rate):\n",
    "        super(CNNModel1, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 64, 2)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.conv5 = nn.Conv2d(64, 32, 2)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.fc1 = nn.Linear(32*5*5, fully_layer_1)\n",
    "        self.fc2 = nn.Linear(fully_layer_1, fully_layer_2)\n",
    "        self.fc3 = nn.Linear(fully_layer_2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "#         Â print(x.shape)\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "#         print(x.shape)\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "#         print(x.shape)\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "#         print(x.shape)\n",
    "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = x.view(-1, 32*5*5)\n",
    "        x = F.dropout(F.relu(self.fc1(x)), self.drop_rate)\n",
    "        x = F.dropout(F.relu(self.fc2(x)), self.drop_rate)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    device = \"cpu\"\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    if use_gpu:\n",
    "        print(\"GPU is available on this device!\")\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        print(\"CPU is available on this device!\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def get_loss(model, criterion, data_loader, device):\n",
    "    total_count = 0\n",
    "    total_loss = 0.0\n",
    "    all_comp_ids = []\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for i, data in enumerate(data_loader):\n",
    "        img_arrs, labels, comp_ids = data\n",
    "        img_arrs, labels = torch.tensor(img_arrs).type(torch.FloatTensor).to(device), torch.tensor(labels).to(device)\n",
    "        total_count += len(comp_ids)\n",
    "        y_pred = model(img_arrs).to(device)\n",
    "        loss = criterion(y_pred.squeeze(), labels)\n",
    "        total_loss += float(loss.item())\n",
    "        all_comp_ids.extend(list(comp_ids))\n",
    "        _, preds = torch.max(y_pred, 1)\n",
    "        all_labels.extend(list(labels))\n",
    "        all_predictions.extend(list(preds))\n",
    "\n",
    "\n",
    "    return total_loss, total_count, all_comp_ids, all_labels, all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_test_training(target_id, fully_layer_1, fully_layer_2, learning_rate, batch_size, drop_rate, n_epoch, experiment_name):\n",
    "    arguments = [str(argm) for argm in\n",
    "                 [target_id, fully_layer_1, fully_layer_2, learning_rate, batch_size, drop_rate, n_epoch, experiment_name]]\n",
    "\n",
    "    str_arguments = \"-\".join(arguments)\n",
    "    print(\"Arguments:\", str_arguments)\n",
    "\n",
    "    device = get_device()\n",
    "    \n",
    "#     exp_path = os.path.join(result_files_path, \"experiments\", experiment_name)\n",
    "#     if not os.path.exists(exp_path): os.makedirs(exp_path)\n",
    "\n",
    "    train_loader, valid_loader, test_loader = get_dataLoader(target_id, batch_size)\n",
    "        \n",
    "    model = CNNModel1(fully_layer_1, fully_layer_2, drop_rate).to(device)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "#     best_val_mcc_score, best_test_mcc_score = 0.0, 0.0\n",
    "#     best_val_test_performance_dict = dict()\n",
    "#     best_val_test_performance_dict[\"MCC\"] = 0.0\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        total_training_count = 0\n",
    "        total_training_loss = 0.0\n",
    "        print(\"\\n----------\\nEpoch : {}\".format(epoch))\n",
    "        \n",
    "        model.train()\n",
    "        batch_number = 0\n",
    "        \n",
    "        all_training_labels = []\n",
    "        all_training_preds = []\n",
    "        \n",
    "        print(\"Training mode\")\n",
    "        \n",
    "        for i, data in enumerate(train_loader):\n",
    "            batch_number += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            img_arrs, labels, comp_ids = data\n",
    "            img_arrs, labels = torch.tensor(img_arrs).type(torch.FloatTensor).to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "            total_training_count += len(comp_ids)\n",
    "            \n",
    "            y_pred = model(img_arrs).to(device)\n",
    "            \n",
    "            _, preds = torch.max(y_pred, 1)\n",
    "            all_training_labels.extend(list(labels))\n",
    "            all_training_preds.extend(list(preds))\n",
    "\n",
    "            loss = criterion(y_pred.squeeze(), labels)\n",
    "            total_training_loss += float(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Epoch : {} | Loss: {}\".format(epoch,total_training_loss))\n",
    "        \n",
    "        training_perf_dict = dict()\n",
    "        \n",
    "        try: training_perf_dict = prec_rec_f1_acc_mcc(all_training_labels, all_training_preds)\n",
    "        except: print(\"There was a problem during training performance calculation!\")\n",
    "            \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():  \n",
    "            \n",
    "            print(\"Validation mode\")\n",
    "\n",
    "            total_val_loss, total_val_count, all_val_comp_ids, all_val_labels, val_predictions = get_loss (model, criterion, valid_loader, device)\n",
    "            \n",
    "            val_perf_dict = dict()\n",
    "            val_perf_dict[\"MCC\"] = 0.0\n",
    "            \n",
    "            try: val_perf_dict = prec_rec_f1_acc_mcc(all_val_labels, val_predictions)\n",
    "            except: print(\"There was a problem during validation performance calculation!\")\n",
    "            \n",
    "            total_test_loss, total_test_count, all_test_comp_ids, all_test_labels, test_predictions = get_loss (\n",
    "                model, criterion, test_loader, device)\n",
    "            \n",
    "            test_perf_dict = dict()\n",
    "            test_perf_dict[\"MCC\"] = 0.0\n",
    "            \n",
    "            try: test_perf_dict = prec_rec_f1_acc_mcc(all_test_labels, test_predictions)\n",
    "            except: print(\"There was a problem during test performance calculation!\")\n",
    "\n",
    "\n",
    "        if epoch == n_epoch - 1:\n",
    "            score_list = get_list_of_scores()\n",
    "            print(\"Training scores: {}\\n\\nValidation scores: {}\\n\".format(training_perf_dict, val_perf_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_test_training(\"CHEMBL286\",  512, 256, 0.001, 32,\n",
    "                               0.25, 10, \"my_experiment\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "OUTPUT IN VM INSTANCE:\n",
    "    \n",
    "    \n",
    "Arguments: CHEMBL286-512-256-0.001-32-0.25-10-my_experiment\n",
    "CPU is available on this device!\n",
    "Fetching training dataset for Target: CHEMBL286\n",
    "Fetching validation dataset for Target: CHEMBL286\n",
    "Fetching test dataset for Target: CHEMBL286\n",
    "\n",
    "----------\n",
    "Epoch : 0\n",
    "Training mode\n",
    "Epoch : 0 | Loss: 24.623343855142593\n",
    "Validation mode\n",
    "\n",
    "\n",
    "----------\n",
    "Epoch : 1\n",
    "Training mode\n",
    "Epoch : 1 | Loss: 21.871842563152313\n",
    "Validation mode\n",
    "\n",
    "----------\n",
    "Epoch : 2\n",
    "Training mode\n",
    "Epoch : 2 | Loss: 18.412213176488876\n",
    "Validation mode\n",
    "\n",
    "----------\n",
    "Epoch : 3\n",
    "Training mode\n",
    "Epoch : 3 | Loss: 17.25254960358143\n",
    "Validation mode\n",
    "\n",
    "----------\n",
    "Epoch : 4\n",
    "Training mode\n",
    "Epoch : 4 | Loss: 16.09722039103508\n",
    "Validation mode\n",
    "\n",
    "----------\n",
    "Epoch : 5\n",
    "Training mode\n",
    "Epoch : 5 | Loss: 14.411342665553093\n",
    "Validation mode\n",
    "\n",
    "----------\n",
    "Epoch : 6\n",
    "Training mode\n",
    "Epoch : 6 | Loss: 13.260539785027504\n",
    "Validation mode\n",
    "\n",
    "----------\n",
    "Epoch : 7\n",
    "Training mode\n",
    "Epoch : 7 | Loss: 13.358637064695358\n",
    "Validation mode\n",
    "\n",
    "----------\n",
    "Epoch : 8\n",
    "Training mode\n",
    "Epoch : 8 | Loss: 12.603277996182442\n",
    "Validation mode\n",
    "\n",
    "----------\n",
    "Epoch : 9\n",
    "Training mode\n",
    "Epoch : 9 | Loss: 12.466190099716187\n",
    "Validation mode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Training scores: {'Precision': 0.8565840938722294, 'Recall': 0.9253521126760563, 'F1-Score': 0.8896411645226812, 'Accuracy': 0.8622147083685545, 'MCC': 0.7107543042098888, 'TP': 657, 'FP': 110, 'TN': 363, 'FN': 53}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Validation scores: {'Precision': 0.9161290322580645, 'Recall': 0.797752808988764, 'F1-Score': 0.8528528528528528, 'Accuracy': 0.835016835016835, 'MCC': 0.6754343641638585, 'TP': 142, 'FP': 13, 'TN': 106, 'FN': 36}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF LEARN MODEL -> CONVERT TO KERAS\n",
    "\n",
    "# from tflearn.activations import relu\n",
    "# from tflearn.layers.conv import avg_pool_2d, conv_2d, max_pool_2d\n",
    "\n",
    "# def CNNModel(outnode, model_name,  target, opt, learn_r, epch, n_of_h1, dropout_keep_rate, save_model=False):\n",
    "#     convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "\n",
    "#     convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "#     convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "#     convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "#     convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "#     convnet = conv_2d(convnet, 128, 5, activation='relu')\n",
    "#     convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "#     convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "#     convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "#     convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "#     convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "#     convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "#     convnet = dropout(convnet, 0.8)\n",
    "\n",
    "#     convnet = fully_connected(convnet, outnode, activation='softmax')\n",
    "#     convnet = regression(convnet, optimizer=opt, learning_rate=learn_r, loss='categorical_crossentropy', name='targets')\n",
    "\n",
    "#     str_model_name = \"{}_{}_{}_{}_{}_{}_{}_{}\".format(model_name,  target, opt, learn_r, epch, n_of_h1, dropout_keep_rate, save_model)\n",
    "\n",
    "#     model = None\n",
    "\n",
    "#     if save_model:\n",
    "#         print(\"Model will be saved!\")\n",
    "#         model = tflearn.DNN(convnet, checkpoint_path='../tflearnModels/{}'.format(str_model_name), best_checkpoint_path='../tflearnModels/bestModels/best_{}'.format(str_model_name),\n",
    "#                         max_checkpoints=1, tensorboard_verbose=0, tensorboard_dir=\"../tflearnLogs/{}/\".format(str_model_name))\n",
    "#     else:\n",
    "#         model = tflearn.DNN(convnet)\n",
    "\n",
    "#     return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
