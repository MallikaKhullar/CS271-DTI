{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, BatchSampler, SequentialSampler\n",
    "\n",
    "project_file_path = \"{}CS271-DTI\".format(os.getcwd().split(\"CS271-DTI\")[0])\n",
    "training_files_path = \"{}/training_files\".format(project_file_path)\n",
    "\n",
    "# print(project_file_path, training_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataSet(Dataset):\n",
    "    def __init__(self, target_id, train_val_test):\n",
    "        print(\"Fetching {} dataset for Target: {}\".format(train_val_test, target_id))\n",
    "        self.target_id = target_id\n",
    "        self.train_val_test = train_val_test\n",
    "        self.training_dataset_path = \"{}/target_training_datasets/{}\".format(training_files_path, target_id)\n",
    "\n",
    "        #         print(self.training_dataset_path)\n",
    "\n",
    "        self.train_val_test_folds = json.load(open(os.path.join(self.training_dataset_path, \"train_val_test_dict.json\")))\n",
    "        self.compid_list = [compid_label[0] for compid_label in self.train_val_test_folds[train_val_test]]\n",
    "        self.label_list = [compid_label[1] for compid_label in self.train_val_test_folds[train_val_test]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.compid_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comp_id = self.compid_list[index]\n",
    "        img_path = os.path.join(self.training_dataset_path, \"imgs\", \"{}.png\".format(comp_id))\n",
    "        img_arr = cv2.imread(img_path)\n",
    "        if random.random()>=0.50:\n",
    "            angle = random.randint(0,359)\n",
    "            rows, cols, channel = img_arr.shape\n",
    "            rotation_matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "            img_arr = cv2.warpAffine(img_arr, rotation_matrix, (cols, rows), cv2.INTER_LINEAR,\n",
    "                                                 borderValue=(255, 255, 255))\n",
    "        img_arr = np.array(img_arr) / 255.0\n",
    "        img_arr = img_arr.transpose((2, 0, 1))\n",
    "        label = self.label_list[index]\n",
    "        return img_arr, label, comp_id\n",
    "\n",
    "\n",
    "def get_dataLoader(target_id, batch_size=32):\n",
    "    training_dataset = DataSet(target_id, \"training\")\n",
    "    validation_dataset = DataSet(target_id, \"validation\")\n",
    "    test_dataset = DataSet(target_id, \"test\")\n",
    "    train_sampler = SubsetRandomSampler(range(len(training_dataset)))\n",
    "    train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size,\n",
    "                                              sampler=train_sampler)\n",
    "\n",
    "    validation_sampler = SubsetRandomSampler(range(len(validation_dataset)))\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size,\n",
    "                                               sampler=validation_sampler)\n",
    "\n",
    "    test_sampler = SubsetRandomSampler(range(len(test_dataset)))\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                               sampler=test_sampler)\n",
    "\n",
    "    return train_loader, validation_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from operator import itemgetter\n",
    "\n",
    "class CNNModel1(nn.Module):\n",
    "    def __init__(self, fully_layer_1, fully_layer_2, drop_rate):\n",
    "        super(CNNModel1, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 64, 2)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.conv5 = nn.Conv2d(64, 32, 2)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.fc1 = nn.Linear(32*5*5, fully_layer_1)\n",
    "        self.fc2 = nn.Linear(fully_layer_1, fully_layer_2)\n",
    "        self.fc3 = nn.Linear(fully_layer_2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "#         Â print(x.shape)\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "#         print(x.shape)\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "#         print(x.shape)\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "#         print(x.shape)\n",
    "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = x.view(-1, 32*5*5)\n",
    "        x = F.dropout(F.relu(self.fc1(x)), self.drop_rate)\n",
    "        x = F.dropout(F.relu(self.fc2(x)), self.drop_rate)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_device():\n",
    "    device = \"cpu\"\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    if use_gpu:\n",
    "        print(\"GPU is available on this device!\")\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        print(\"CPU is available on this device!\")\n",
    "    return device\n",
    "\n",
    "\n",
    "\n",
    "def get_loss(model, criterion, data_loader, device):\n",
    "    total_count = 0\n",
    "    total_loss = 0.0\n",
    "    all_comp_ids = []\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for i, data in enumerate(data_loader):\n",
    "        img_arrs, labels, comp_ids = data\n",
    "        img_arrs, labels = torch.tensor(img_arrs).type(torch.FloatTensor).to(device), torch.tensor(labels).to(device)\n",
    "        total_count += len(comp_ids)\n",
    "        y_pred = model(img_arrs).to(device)\n",
    "        loss = criterion(y_pred.squeeze(), labels)\n",
    "        total_loss += float(loss.item())\n",
    "        all_comp_ids.extend(list(comp_ids))\n",
    "        _, preds = torch.max(y_pred, 1)\n",
    "        all_labels.extend(list(labels))\n",
    "        all_predictions.extend(list(preds))\n",
    "\n",
    "\n",
    "    return total_loss, total_count, all_comp_ids, all_labels, all_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_test_training(target_id, fully_layer_1, fully_layer_2, learning_rate, batch_size, drop_rate, n_epoch, experiment_name):\n",
    "    arguments = [str(argm) for argm in\n",
    "                 [target_id, fully_layer_1, fully_layer_2, learning_rate, batch_size, drop_rate, n_epoch, experiment_name]]\n",
    "\n",
    "    str_arguments = \"-\".join(arguments)\n",
    "    print(\"Arguments:\", str_arguments)\n",
    "\n",
    "    device = get_device()\n",
    "    \n",
    "#     exp_path = os.path.join(result_files_path, \"experiments\", experiment_name)\n",
    "#     if not os.path.exists(exp_path): os.makedirs(exp_path)\n",
    "\n",
    "    train_loader, valid_loader, test_loader = get_dataLoader(target_id, batch_size)\n",
    "        \n",
    "    model = CNNModel1(fully_layer_1, fully_layer_2, drop_rate).to(device)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "#     best_val_mcc_score, best_test_mcc_score = 0.0, 0.0\n",
    "#     best_val_test_performance_dict = dict()\n",
    "#     best_val_test_performance_dict[\"MCC\"] = 0.0\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        total_training_count = 0\n",
    "        total_training_loss = 0.0\n",
    "        print(\"Epoch :{}\".format(epoch))\n",
    "        \n",
    "        model.train()\n",
    "        batch_number = 0\n",
    "        \n",
    "        all_training_labels = []\n",
    "        all_training_preds = []\n",
    "        \n",
    "        print(\"Training mode\")\n",
    "        \n",
    "        for i, data in enumerate(train_loader):\n",
    "            batch_number += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            img_arrs, labels, comp_ids = data\n",
    "            img_arrs, labels = torch.tensor(img_arrs).type(torch.FloatTensor).to(device), torch.tensor(labels).to(device)\n",
    "\n",
    "            total_training_count += len(comp_ids)\n",
    "            \n",
    "            y_pred = model(img_arrs).to(device)\n",
    "            \n",
    "            _, preds = torch.max(y_pred, 1)\n",
    "            all_training_labels.extend(list(labels))\n",
    "            all_training_preds.extend(list(preds))\n",
    "\n",
    "            loss = criterion(y_pred.squeeze(), labels)\n",
    "            total_training_loss += float(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Epoch : {} | Loss: {}\".format(epoch,total_training_loss))\n",
    "        \n",
    "        training_perf_dict = dict()\n",
    "        \n",
    "        try: training_perf_dict = prec_rec_f1_acc_mcc(all_training_labels, all_training_preds)\n",
    "        except: print(\"There was a problem during training performance calculation!\")\n",
    "            \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():  \n",
    "            \n",
    "            print(\"Validation mode\")\n",
    "\n",
    "            total_val_loss, total_val_count, all_val_comp_ids, all_val_labels, val_predictions = get_loss (model, criterion, valid_loader, device)\n",
    "            \n",
    "            val_perf_dict = dict()\n",
    "            val_perf_dict[\"MCC\"] = 0.0\n",
    "            \n",
    "            try: val_perf_dict = prec_rec_f1_acc_mcc(all_val_labels, val_predictions)\n",
    "            except: print(\"There was a problem during validation performance calculation!\")\n",
    "            \n",
    "            total_test_loss, total_test_count, all_test_comp_ids, all_test_labels, test_predictions = get_loss (\n",
    "                model, criterion, test_loader, device)\n",
    "            \n",
    "            test_perf_dict = dict()\n",
    "            test_perf_dict[\"MCC\"] = 0.0\n",
    "            \n",
    "            try: test_perf_dict = prec_rec_f1_acc_mcc(all_test_labels, test_predictions)\n",
    "            except: print(\"There was a problem during test performance calculation!\")\n",
    "\n",
    "\n",
    "        if epoch == n_epoch - 1:\n",
    "            score_list = get_list_of_scores()\n",
    "            print(\"Training scores: {}\\n\\nValidation scores: {}\\n\".format(training_perf_dict, val_perf_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: CHEMBL286-512-256-0.001-32-0.25-10-my_experiment\n",
      "CPU is available on this device!\n",
      "Fetching training dataset for Target: CHEMBL286\n",
      "Fetching validation dataset for Target: CHEMBL286\n",
      "Fetching test dataset for Target: CHEMBL286\n",
      "Epoch :0\n",
      "Training mode:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mallikapriyakhullar/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0 training loss:0.7471051812171936\n",
      "There was a problem during training performance calculation!\n",
      "Validation mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mallikapriyakhullar/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a problem during validation performance calculation!\n",
      "There was a problem during test performance calculation!\n",
      "Epoch :1\n",
      "Training mode:\n",
      "Epoch :1 training loss:0.6462336182594299\n",
      "There was a problem during training performance calculation!\n",
      "Validation mode\n",
      "There was a problem during validation performance calculation!\n",
      "There was a problem during test performance calculation!\n",
      "Epoch :2\n",
      "Training mode:\n",
      "Epoch :2 training loss:0.7233472466468811\n",
      "There was a problem during training performance calculation!\n",
      "Validation mode\n",
      "There was a problem during validation performance calculation!\n",
      "There was a problem during test performance calculation!\n",
      "Epoch :3\n",
      "Training mode:\n",
      "Epoch :3 training loss:0.5956683158874512\n",
      "There was a problem during training performance calculation!\n",
      "Validation mode\n",
      "There was a problem during validation performance calculation!\n",
      "There was a problem during test performance calculation!\n",
      "Epoch :4\n",
      "Training mode:\n",
      "Epoch :4 training loss:0.6022623181343079\n",
      "There was a problem during training performance calculation!\n",
      "Validation mode\n",
      "There was a problem during validation performance calculation!\n",
      "There was a problem during test performance calculation!\n",
      "Epoch :5\n",
      "Training mode:\n",
      "Epoch :5 training loss:0.47276976704597473\n",
      "There was a problem during training performance calculation!\n",
      "Validation mode\n",
      "There was a problem during validation performance calculation!\n",
      "There was a problem during test performance calculation!\n",
      "Epoch :6\n",
      "Training mode:\n",
      "Epoch :6 training loss:0.5571061968803406\n",
      "There was a problem during training performance calculation!\n",
      "Validation mode\n",
      "There was a problem during validation performance calculation!\n",
      "There was a problem during test performance calculation!\n",
      "Epoch :7\n",
      "Training mode:\n",
      "Epoch :7 training loss:0.4662226438522339\n",
      "There was a problem during training performance calculation!\n",
      "Validation mode\n",
      "There was a problem during validation performance calculation!\n",
      "There was a problem during test performance calculation!\n",
      "Epoch :8\n",
      "Training mode:\n",
      "Epoch :8 training loss:0.45767438411712646\n",
      "There was a problem during training performance calculation!\n",
      "Validation mode\n",
      "There was a problem during validation performance calculation!\n",
      "There was a problem during test performance calculation!\n",
      "Epoch :9\n",
      "Training mode:\n",
      "Epoch :9 training loss:0.2951495349407196\n",
      "There was a problem during training performance calculation!\n",
      "Validation mode\n",
      "There was a problem during validation performance calculation!\n",
      "There was a problem during test performance calculation!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_list_of_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a54bdfefc857>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_validation_test_training(\"CHEMBL286\",  512, 256, 0.001, 32,\n\u001b[0;32m----> 2\u001b[0;31m                                0.25, 10, \"my_experiment\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-06479ad62096>\u001b[0m in \u001b[0;36mtrain_validation_test_training\u001b[0;34m(target_id, fully_layer_1, fully_layer_2, learning_rate, batch_size, drop_rate, n_epoch, experiment_name)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mscore_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_list_of_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training scores: {}\\n\\nValidation scores: {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_perf_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_perf_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_list_of_scores' is not defined"
     ]
    }
   ],
   "source": [
    "train_validation_test_training(\"CHEMBL286\",  512, 256, 0.001, 32,\n",
    "                               0.25, 10, \"my_experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF LEARN MODEL -> CONVERT TO KERAS\n",
    "\n",
    "# from tflearn.activations import relu\n",
    "# from tflearn.layers.conv import avg_pool_2d, conv_2d, max_pool_2d\n",
    "\n",
    "# def CNNModel(outnode, model_name,  target, opt, learn_r, epch, n_of_h1, dropout_keep_rate, save_model=False):\n",
    "#     convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "\n",
    "#     convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "#     convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "#     convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "#     convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "#     convnet = conv_2d(convnet, 128, 5, activation='relu')\n",
    "#     convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "#     convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "#     convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "#     convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "#     convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "#     convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "#     convnet = dropout(convnet, 0.8)\n",
    "\n",
    "#     convnet = fully_connected(convnet, outnode, activation='softmax')\n",
    "#     convnet = regression(convnet, optimizer=opt, learning_rate=learn_r, loss='categorical_crossentropy', name='targets')\n",
    "\n",
    "#     str_model_name = \"{}_{}_{}_{}_{}_{}_{}_{}\".format(model_name,  target, opt, learn_r, epch, n_of_h1, dropout_keep_rate, save_model)\n",
    "\n",
    "#     model = None\n",
    "\n",
    "#     if save_model:\n",
    "#         print(\"Model will be saved!\")\n",
    "#         model = tflearn.DNN(convnet, checkpoint_path='../tflearnModels/{}'.format(str_model_name), best_checkpoint_path='../tflearnModels/bestModels/best_{}'.format(str_model_name),\n",
    "#                         max_checkpoints=1, tensorboard_verbose=0, tensorboard_dir=\"../tflearnLogs/{}/\".format(str_model_name))\n",
    "#     else:\n",
    "#         model = tflearn.DNN(convnet)\n",
    "\n",
    "#     return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
